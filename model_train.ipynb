{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"model_train.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\giria\\anaconda3\\envs\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\giria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# The F1 score can be interpreted as a harmonic mean of precision. F1 achieves its best value at 1 and its worst score at 0. \n",
    "# Accuracy_score is used to calculate the accuracy of the fraction or number of correct predictions.\n",
    "# Precision is a measure of the relevance of the results, while recall is a measure of the number of actually relevant results returned.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "from datasets import load_dataset, set_caching_enabled\n",
    "from datasets.arrow_dataset import DatasetTransformationNotAllowedError\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    # Preprocessing\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    # Text and image models (now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
    "    AutoModel,            \n",
    "    # Training / Evaluation\n",
    "    TrainingArguments, Trainer,\n",
    "    # OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "    logging )\n",
    "# nltk is an open-source library written in Python dedicated to natural language processing (English language)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# WordNet is a lexical database of semantic relations between words in more than 200 languages. \n",
    "# WordNet links words in semantic relations including synonyms, hyponyms, and meronyms.\n",
    "\n",
    "\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATHERING OUR DATA: \n",
    "\n",
    "'Our Capstone Project Data will be splitted into three lists,'\n",
    "\n",
    "* 1. Training Data with 12792 rows \n",
    "* 2. Validation Data with 2000 rows \n",
    "* 3. Testing Data with 500 rows and questions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    training: Dataset({\n",
       "        features: ['img_id', 'question', 'answer', 'label'],\n",
       "        num_rows: 12792\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['img_id', 'question', 'answer', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img_id', 'question', 'answer', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and instantiate the training and evaluation datasets present in CSV format\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files={\n",
    "        \"training\": os.path.join('data_train.csv'),\n",
    "        \"validation\": os.path.join('data_val.csv'),\n",
    "        \"test\": os.path.join('data_test.csv')\n",
    "    }\n",
    ")\n",
    "# Load the space of all possible answers\n",
    "with open(os.path.join('answer_space.txt')) as f:\n",
    "    answer_space = f.read().splitlines()\n",
    "\n",
    "# Since we are modeling the VQA task as a multiclass classification problem,\n",
    "# we need to create labels from the actual answers\n",
    "dataset = dataset.map(\n",
    "    lambda examples: {\n",
    "        'label': [\n",
    "            answer_space.index(ans) for ans in examples['answer']           \n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI MODAL COLLATOR: \n",
    "\n",
    "In our multi modal collator class we implement two transformers: \n",
    "* 1. AutoTokenizer: That helps to convert raw questions sentence  into inputs \n",
    "* 2. AutoFeatureExtractor that helps to convert our raw images into inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    # We use AutoTokenizer and AutoFeatureExtractor from transformers to convert raw images and questions into inputs.\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoFeatureExtractor\n",
    "\n",
    "    def tokenize_text(self, texts: List[str]):\n",
    "      \"\"\"The tokenize_text function specifically returns a dictionary of values instead of a simple list of values.\"\"\"\n",
    "      encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            # PyTorch represents data as multi-dimensional arrays, similar to NumPy arrays, called “Tensor”.\n",
    "            return_tensors='pt', # return_tensors = \"pt\" is just so that the tokenizer returns PyTorch tensors.\n",
    "            # Batch inputs are often of different lengths, padding and truncation are strategies to handle this issue.\n",
    "            # Padding adds a special padding token to ensure that shorter sequences will have the same length as the longest sequence\n",
    "            # in a batch or the maximum length accepted by the model (512).\n",
    "            padding='longest',\n",
    "            # truncation works in the opposite direction of padding\n",
    "            truncation=True,\n",
    "            max_length=24,\n",
    "            # sequences will be encoded with the special tokens related to their model.\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids=True,\n",
    "            # The attention mask is a binary tensor indicating the position of padding indices so that the model does not attend to them.\n",
    "            # For the BertTokenizer, 1 indicates a value that should be attended to, while 0 indicates a padded value\n",
    "            return_attention_mask=True,         \n",
    "        )\n",
    "      return {\n",
    "            # The squeeze function reduces the length 1 dimensions of the tensor\n",
    "            # For example, for the following input shape: AX1XBXCX1XD, the function squeeze(input) would return the output AXBXCXD\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "    def preprocess_images(self, images: List[str]):\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[Image.open(os.path.join('train\\Train_images', img_id + \".jpg\")).convert('RGB') for img_id in images],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "            \n",
    "    def __call__(self, raw_batch_dict):\n",
    "        return {\n",
    "            # The isinstance() function checks if the object (raw_batch_dict) is an instance or subclass of the dict class (second argument).\n",
    "            **self.tokenize_text(\n",
    "                raw_batch_dict['question']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['question'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            **self.preprocess_images(\n",
    "                raw_batch_dict['img_id']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['img_id'] for i in raw_batch_dict]\n",
    "            ),\n",
    "            'labels': torch.tensor(\n",
    "                raw_batch_dict['label']\n",
    "                if isinstance(raw_batch_dict, dict) else\n",
    "                [i['label'] for i in raw_batch_dict],\n",
    "                dtype=torch.int64\n",
    "            ),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Visual Question Answering (VQA) Model\n",
    "\n",
    "## Overview\n",
    "This PyTorch module, `MultimodalVQAModel`, is designed for Visual Question Answering tasks. It leverages pretrained models for processing both textual and visual inputs, fuses these two modalities, and performs classification to provide answers to visual questions. The class inherits from `nn.Module`, the base class for all neural network modules in PyTorch.\n",
    "\n",
    "## Components\n",
    "- **Text and Image Encoders**: Utilizes `AutoModel.from_pretrained` to load pretrained models specified by `pretrained_text_name` and `pretrained_image_name`. These encoders are used to process the text and image inputs, respectively.\n",
    "\n",
    "- **Fusion Layer**: A sequential module that combines the outputs from the text and image encoders, processes them through a linear layer, applies a ReLU activation, and finally uses dropout for regularization.\n",
    "\n",
    "- **Classifier**: A linear layer that takes the fused representation and outputs logits over the possible answers.\n",
    "\n",
    "- **Loss Function**: Utilizes CrossEntropyLoss for training the model.\n",
    "\n",
    "## Initialization Parameters\n",
    "- `pretrained_text_name` (str): Identifier for the pretrained text model to be loaded.\n",
    "- `pretrained_image_name` (str): Identifier for the pretrained image model to be loaded.\n",
    "- `num_labels` (int): Number of possible answers or labels. Defaults to the length of `answer_space`.\n",
    "- `intermediate_dim` (int): Dimension of the fused representation. Defaults to 512.\n",
    "- `dropout` (float): Dropout rate for regularization. Defaults to 0.5.\n",
    "\n",
    "## Forward Pass Arguments\n",
    "- `input_ids` (torch.LongTensor): Tokenized text input.\n",
    "- `pixel_values` (torch.FloatTensor): Preprocessed image input.\n",
    "- `attention_mask` (Optional[torch.LongTensor]): Mask to avoid attention on padding token indices for text.\n",
    "- `token_type_ids` (Optional[torch.LongTensor]): Segment token indices to indicate first and second portions of the inputs for models that require them.\n",
    "- `labels` (Optional[torch.LongTensor]): True labels for computing loss during training.\n",
    "\n",
    "## Outputs\n",
    "- A dictionary containing:\n",
    "  - `logits`: The classification logits.\n",
    "  - `loss`: The computed loss, returned only if `labels` is provided.\n",
    "\n",
    "## Usage\n",
    "The model can be instantiated with specific pretrained models for text and image processing. It is designed to work within a training and evaluation framework, where it can be trained on a dataset of visual questions and images, evaluated for accuracy and other metrics, and used to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "# nn.Module is the base class for PyTorch's neural network\n",
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(self, pretrained_text_name, pretrained_image_name, num_labels=len(answer_space), intermediate_dim=512, dropout=0.5):\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "\n",
    "        # Text and image encoders\n",
    "        self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
    "\n",
    "        # Fusion layer\n",
    "        # The \"Sequential\" module is a \"container\" module that allows defining a feed-forward network\n",
    "        self.fusion = nn.Sequential(\n",
    "            # text_encoder.config.hidden_size allows us to get the size of the raw outputs from the text encoder\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            # the ReLU (Rectified Linear Unit) function is an activation function that filters our data.\n",
    "            # It passes positive values (x > 0) to the following layers of the neural network.\n",
    "            nn.ReLU(),\n",
    "            # Dropout method involves randomly deactivating neuron outputs (0.5 for hidden layers)\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Forward pass as a method\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor,\n",
    "            pixel_values: torch.FloatTensor,\n",
    "            attention_mask: Optional[torch.LongTensor] = None,\n",
    "            token_type_ids: Optional[torch.LongTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None):\n",
    "        \n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(fused_output)\n",
    "        \n",
    "        out = {\n",
    "            \"logits\": logits\n",
    "        }\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Create Multimodal VQA Collator and Model\n",
    "\n",
    "##### Overview\n",
    "This function, `createMultimodalVQACollatorAndModel`, initializes and returns a data collator and a multimodal Visual Question Answering (VQA) model for use in training and evaluation. It specifically prepares the components for handling both text and image inputs.\n",
    "\n",
    "###### Parameters\n",
    "- `text` (str): Identifier for the pretrained text model. Defaults to `'dmis-lab/biobert-v1.1'`.\n",
    "- `image` (str): Identifier for the pretrained image model. Defaults to `'microsoft/swin-tiny-patch4-window7-224'`.\n",
    "\n",
    "##### Returns\n",
    "- `multi_collator`: An instance of `MultimodalCollator`, configured with the specified text tokenizer and image feature extractor.\n",
    "- `multi_model`: An instance of `MultimodalVQAModel`, prepared with the given pretrained text and image models, ready for training or inference.\n",
    "\n",
    "##### Functionality\n",
    "1. **Tokenizer and Feature Extractor Initialization**: Loads the specified tokenizer for text and feature extractor for images.\n",
    "2. **Collator Creation**: Combines the tokenizer and feature extractor into a collator that prepares batches of data.\n",
    "3. **Model Initialization**: Constructs the multimodal model with the specified text and image encoders.\n",
    "\n",
    "##### Usage\n",
    "This setup is essential for multimodal learning tasks where both textual questions and visual inputs are involved, facilitating seamless preprocessing, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoFeatureExtractor\n",
    "\n",
    "def createMultimodalVQACollatorAndModel(text='dmis-lab/biobert-v1.1', image='microsoft/swin-tiny-patch4-window7-224'):\n",
    "    # Initialize the correct text tokenizer and image feature extractor, and use them to create the collator (Dataloader)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n",
    "    multi_collator = MultimodalCollator(tokenizer=tokenizer, preprocessor=preprocessor)\n",
    "    \n",
    "    # Instantiate and initialize the multimodal model with the appropriate pretrained models\n",
    "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
    "    \n",
    "    return multi_collator, multi_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu-Palmer Similarity Measure Function\n",
    "\n",
    "#### Overview\n",
    "Calculates the Wu-Palmer similarity score between two words using WordNet's synsets, facilitating semantic similarity assessment in NLP tasks.\n",
    "\n",
    "#### Functions\n",
    "\n",
    "- **wup_measure(a, b, similarity_threshold=0.925)**: Main function to compute the Wu-Palmer similarity score between two words `a` and `b`. It considers the best semantic field match for each word and applies a similarity threshold to adjust the weighting.\n",
    "\n",
    "- **get_semantic_field(word)**: Retrieves the semantic field (synsets) for a given word, used to interpret its meanings in different contexts.\n",
    "\n",
    "- **get_stem_word(word)**: Extracts the base word and its weight, especially handling cases where the word comes with an additional identifier (e.g., `word\\d+:wordid`).\n",
    "\n",
    "#### Process\n",
    "1. **Initialization**: Sets a global weight and extracts the stem word for both input words.\n",
    "2. **Semantic Field Retrieval**: Gets the semantic fields for both words.\n",
    "3. **Similarity Calculation**: Computes the Wu-Palmer similarity scores across all semantic field combinations of the two words.\n",
    "4. **Final Score**: Adjusts the final score based on similarity thresholds and returns the weighted score.\n",
    "\n",
    "This method offers a nuanced way to compare word meanings based on their positions within a hierarchical structure like WordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def wup_measure(a, b, similarity_threshold=0.925):\n",
    "    \"\"\"\n",
    "    Returns the Wu-Palmer similarity score.\n",
    "    Specifically, it computes:\n",
    "    max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
    "    where interp is a 'field of interpretation'\n",
    "    \"\"\"\n",
    "    def get_semantic_field(word):\n",
    "        \"\"\"\n",
    "        Retrieves the semantic field (synsets) for the given word.\n",
    "        \"\"\"\n",
    "        semantic_field = wordnet.synsets(word, pos=wordnet.NOUN)\n",
    "        weight = 1.0  # Initial weight is set to 1.0 for all words\n",
    "        return semantic_field, weight\n",
    "\n",
    "    def get_stem_word(word):\n",
    "        \"\"\"\n",
    "        Sometimes, the answer is in the form word\\d+:wordid.\n",
    "        In that case, we return the word and down-weight it.\n",
    "        \"\"\"\n",
    "        weight = 1.0  # Initial weight, assuming no special weighting is needed\n",
    "        return word, weight\n",
    "\n",
    "    global_weight = 1.0  # Initialize global weight to 1.0\n",
    "\n",
    "    a, global_weight_a = get_stem_word(a)\n",
    "    b, global_weight_b = get_stem_word(b)\n",
    "    global_weight = min(global_weight_a, global_weight_b)  # Use the smaller of the two global weights\n",
    "\n",
    "    if a == b:\n",
    "        # The words are the same\n",
    "        return 1.0 * global_weight\n",
    "\n",
    "    if not a or not b:\n",
    "        # If either word is empty, return 0\n",
    "        return 0\n",
    "\n",
    "    interp_a, weight_a = get_semantic_field(a)\n",
    "    interp_b, weight_b = get_semantic_field(b)\n",
    "\n",
    "    if not interp_a or not interp_b:\n",
    "        # If either word has no semantic field, return 0\n",
    "        return 0\n",
    "\n",
    "    # Take the most optimistic interpretation\n",
    "    global_max = 0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score = x.wup_similarity(y)\n",
    "            if local_score and local_score > global_max:\n",
    "                global_max = local_score\n",
    "\n",
    "    # Adjust weighting based on the semantic fields, unless the score is high, indicating synonyms\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score = global_max * weight_a * weight_b * interp_weight * global_weight\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assuming wup_measure is defined as shown earlier and answer_space is a predefined dictionary mapping indices to words\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    \"\"\"\n",
    "    Wrapper around the wup_measure function to process inputs in batches.\n",
    "    Calculates the Wu-Palmer similarity scores for pairs of labels and predictions,\n",
    "    then returns the average score for the batch.\n",
    "\n",
    "    :param labels: A list of indices representing the correct answers.\n",
    "    :param preds: A list of indices representing the predicted answers.\n",
    "    :return: The average Wu-Palmer similarity score for the batch.\n",
    "    \"\"\"\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# Assuming batch_wup_measure is defined as shown previously\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Function to calculate all relevant performance metrics, to be passed to the trainer.\n",
    "    This function calculates and returns a dictionary containing metrics such as WUPS,\n",
    "    accuracy, precision, recall, and F1 score, with precision, recall, and F1 score multiplied by 10.\n",
    "\n",
    "    :param eval_tuple: A tuple containing an array of logits and an array of labels.\n",
    "    :return: A dictionary with keys as metric names and values as the calculated metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_tuple  # Unpack the tuple into logits and labels\n",
    "    preds = logits.argmax(axis=-1)  # Predictions are derived from logits\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"wups\": batch_wup_measure(labels, preds),  # WUPS calculation\n",
    "        \"accuracy\": accuracy_score(labels, preds),  # Accuracy calculation\n",
    "        \"precision\": precision_score(labels, preds, average='macro') * 10,  # Precision calculation\n",
    "        \"recall\": recall_score(labels, preds, average='macro') * 10,  # Recall calculation\n",
    "        \"f1\": f1_score(labels, preds, average='macro') * 10,  # F1 score calculation\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Multimodal VQA Model\n",
    "\n",
    "### Training Setup\n",
    "- **Arguments**: Configurations for training include epochs, batch sizes, learning rate, and strategies for logging, evaluation, and saving checkpoints.\n",
    "- **Collator & Model**: Initializes using `createMultimodalVQACollatorAndModel` to properly handle multimodal (text and image) inputs.\n",
    "- **Trainer**: Manages training and evaluation processes, utilizing specified datasets, collator, and metrics calculation.\n",
    "\n",
    "### Process Overview\n",
    "1. **Training Loop**: Executes training over 15 epochs, optimizing model weights based on training data.\n",
    "2. **Model Saving**: Limits storage by saving only the top 3 performing checkpoints during training.\n",
    "3. **Evaluation**: After training completion, evaluates the model on a validation set to calculate final metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming the device is already defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Initialize the collator and model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m collator, model \u001b[38;5;241m=\u001b[39m \u001b[43mcreateMultimodalVQACollatorAndModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdmis-lab/biobert-v1.1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmicrosoft/swin-tiny-patch4-window7-224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Prepare DataLoader\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mcreateMultimodalVQACollatorAndModel\u001b[1;34m(text, image)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateMultimodalVQACollatorAndModel\u001b[39m(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdmis-lab/biobert-v1.1\u001b[39m\u001b[38;5;124m'\u001b[39m, image\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/swin-tiny-patch4-window7-224\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Initialize the correct text tokenizer and image feature extractor, and use them to create the collator (Dataloader)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(text)\n\u001b[1;32m----> 6\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoFeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     multi_collator \u001b[38;5;241m=\u001b[39m MultimodalCollator(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, preprocessor\u001b[38;5;241m=\u001b[39mpreprocessor)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Instantiate and initialize the multimodal model with the appropriate pretrained models\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\models\\auto\\feature_extraction_auto.py:343\u001b[0m, in \u001b[0;36mAutoFeatureExtractor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    341\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureExtractionMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_extractor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m feature_extractor_class \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_extractor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    345\u001b[0m feature_extractor_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\feature_extraction_utils.py:498\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m feature_extractor_file \u001b[38;5;241m=\u001b[39m FEATURE_EXTRACTOR_NAME\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 498\u001b[0m     resolved_feature_extractor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_extractor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\utils\\hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1388\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1384\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pointer_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;66;03m# if passed revision is not identical to commit_hash\u001b[39;00m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;66;03m# then revision has to be a branch name or tag name.\u001b[39;00m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;66;03m# In that case store a ref.\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m \u001b[43m_cache_commit_hash_for_specific_revision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_download:\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:949\u001b[0m, in \u001b[0;36m_cache_commit_hash_for_specific_revision\u001b[1;34m(storage_folder, revision, commit_hash)\u001b[0m\n\u001b[0;32m    947\u001b[0m ref_path \u001b[38;5;241m=\u001b[39m Path(storage_folder) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m revision\n\u001b[0;32m    948\u001b[0m ref_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_path\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m commit_hash \u001b[38;5;241m!=\u001b[39m \u001b[43mref_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;66;03m# Update ref only if has been updated. Could cause useless error in case\u001b[39;00m\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;66;03m# repo is already cached and user doesn't have write access to cache folder.\u001b[39;00m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;66;03m# See https://github.com/huggingface/huggingface_hub/issues/1216.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m     ref_path\u001b[38;5;241m.\u001b[39mwrite_text(commit_hash)\n",
      "File \u001b[1;32mc:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\pathlib.py:1058\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1058\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming the device is already defined (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# Initialize the collator and model\n",
    "collator, model = createMultimodalVQACollatorAndModel(text='dmis-lab/biobert-v1.1', image='microsoft/swin-tiny-patch4-window7-224')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collator, num_workers=8)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Optionally, define a scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'multimodal_vqa_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text model loaded successfully.\n",
      "Image model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giria\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor\n",
    "\n",
    "# Test loading the text model\n",
    "try:\n",
    "    text_model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    print(\"Text model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load text model: {e}\")\n",
    "\n",
    "# Test loading the image model\n",
    "try:\n",
    "    image_model = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "    print(\"Image model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load image model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "num_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the collator and model\n",
    "collator, model = createMultimodalVQACollatorAndModel(\n",
    "    text='dmis-lab/biobert-v1.1', \n",
    "    image='microsoft/swin-tiny-patch4-window7-224'\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# DataLoader for training and validation sets\n",
    "train_dataloader = DataLoader(\n",
    "    dataset['training'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collator, \n",
    "    num_workers=8,\n",
    "    pin_memory=True if device.type == \"cuda\" else False  # Pin memory to speed up data transfer to GPU\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset['validation'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collator, \n",
    "    num_workers=8,\n",
    "    pin_memory=True if device.type == \"cuda\" else False\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Optionally, define a scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if v is not None}\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=f\"Epoch {epoch+1} Validation\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if v is not None}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs['loss']\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss / len(train_dataloader)}, Validation Loss = {valid_loss / len(valid_dataloader)}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'multimodal_vqa_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
